{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1 - Batch Ingestion\n",
    "\n",
    "In this exercise you will be ingesting three batches of orders, one for 2017, 2018 and 2019.\n",
    "\n",
    "As each batch is ingested, we are going to append it to a new Delta table, unifying all the datasets into one single dataset.\n",
    "\n",
    "Each year, different individuals and different standards were used resulting in datasets that vary slightly:\n",
    "* In 2017 the backup was written as fixed-width text files\n",
    "* In 2018 the backup was written a tab-separated text files\n",
    "* In 2019 the backup was written as a \"standard\" comma-separted text files but the format of the column names was changed\n",
    "\n",
    "Our only goal here is to unify all the datasets while tracking the source of each record (ingested file name and ingested timestamp) should additional problems arise.\n",
    "\n",
    "Because we are only concerned with ingestion at this stage, the majority of the columns will be ingested as simple strings and in future exercises we will address this issue (and others) with various transformations.\n",
    "\n",
    "This exercise is broken up into 3 steps:\n",
    "* Exercise 1.A - Ingest Fixed-Width File\n",
    "* Exercise 1.B - Ingest Tab-Separated File\n",
    "* Exercise 1.C - Ingest Comma-Separated File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_2017_path = \"data/orders/batch/2017.txt\"\n",
    "batch_2018_path = \"data/orders/batch/2018.csv\"\n",
    "batch_2019_path = \"data/orders/batch/2019.csv\"\n",
    "batch_target_path = \"output/batch_orders_dirty.delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(\"output\"):\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "if os.path.exists(batch_target_path):\n",
    "    shutil.rmtree(batch_target_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #1.A - Ingest Fixed-Width File</h2>\n",
    "\n",
    "**In this step you will need to:**\n",
    "1. Use the variable **`batch_2017_path`**, investigate the 2017 batch file, if needed.\n",
    "2. Configure a **`DataFrameReader`** to ingest the text file identified by **`batch_2017_path`** - this should provide one record per line, with a single column named **`value`**\n",
    "3. Using the information in **`fixed_width_column_defs`** (or the dictionary itself) use the **`value`** column to extract each new column of the appropriate length.<br/>\n",
    "  * The dictionary's key is the column name\n",
    "  * The first element in the dictionary's value is the starting position of that column's data\n",
    "  * The second element in the dictionary's value is the length of that column's data\n",
    "4. Once you are done with the **`value`** column, remove it.\n",
    "5. For each new column created in step #3, remove any leading whitespace\n",
    "  * The introduction of \\[leading\\] white space should be expected when extracting fixed-width values out of the **`value`** column.\n",
    "6. For each new column created in step #3, replace all empty strings with **`null`**.\n",
    "  * After trimming white space, any column for which a value was not specified in the original dataset should result in an empty string.\n",
    "7. Add a new column, **`ingest_file_name`**, which is the name of the file from which the data was read from.\n",
    "  * This should not be hard coded.\n",
    "  * For the proper function, see the <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">pyspark.sql.functions</a> module\n",
    "8. Add a new column, **`ingested_at`**, which is a timestamp of when the data was ingested as a DataFrame.\n",
    "  * This should not be hard coded.\n",
    "  * For the proper function, see the <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">pyspark.sql.functions</a> module\n",
    "9. Write the corresponding **`DataFrame`** in the \"delta\" format to the location specified by **`batch_target_path`**\n",
    "\n",
    "**Special Notes:**\n",
    "* It is possible to use the dictionary **`fixed_width_column_defs`** and programatically extract <br/>\n",
    "  each column but, it is also perfectly OK to hard code this step and extract one column at a time.\n",
    "* The **`SparkSession`** is already provided to you as an instance of **`spark`**.\n",
    "* The classes/methods that you will need for this exercise include:\n",
    "  * **`pyspark.sql.DataFrameReader`** to ingest data\n",
    "  * **`pyspark.sql.DataFrameWriter`** to ingest data\n",
    "  * **`pyspark.sql.Column`** to transform data\n",
    "  * Various functions from the **`pyspark.sql.functions`** module\n",
    "  * Various transformations and actions from **`pyspark.sql.DataFrame`**\n",
    "\n",
    "**Additional Requirements:**\n",
    "* The unified batch dataset must be written to disk in the \"delta\" format\n",
    "* The schema for the unified batch dataset must be:\n",
    "  * **`submitted_at`**:**`string`**\n",
    "  * **`order_id`**:**`string`**\n",
    "  * **`customer_id`**:**`string`**\n",
    "  * **`sales_rep_id`**:**`string`**\n",
    "  * **`sales_rep_ssn`**:**`string`**\n",
    "  * **`sales_rep_first_name`**:**`string`**\n",
    "  * **`sales_rep_last_name`**:**`string`**\n",
    "  * **`sales_rep_address`**:**`string`**\n",
    "  * **`sales_rep_city`**:**`string`**\n",
    "  * **`sales_rep_state`**:**`string`**\n",
    "  * **`sales_rep_zip`**:**`string`**\n",
    "  * **`shipping_address_attention`**:**`string`**\n",
    "  * **`shipping_address_address`**:**`string`**\n",
    "  * **`shipping_address_city`**:**`string`**\n",
    "  * **`shipping_address_state`**:**`string`**\n",
    "  * **`shipping_address_zip`**:**`string`**\n",
    "  * **`product_id`**:**`string`**\n",
    "  * **`product_quantity`**:**`string`**\n",
    "  * **`product_sold_price`**:**`string`**\n",
    "  * **`ingest_file_name`**:**`string`**\n",
    "  * **`ingested_at`**:**`timestamp`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_spark_session\n",
    "\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed-Width Meta Data \n",
    "\n",
    "The following dictionary is provided for reference and/or implementation<br/>\n",
    "(depending on which strategy you choose to employ).\n",
    "\n",
    "Run the following cell to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_width_column_defs = {\n",
    "  \"submitted_at\": (1, 15),\n",
    "  \"order_id\": (16, 40),\n",
    "  \"customer_id\": (56, 40),\n",
    "  \"sales_rep_id\": (96, 40),\n",
    "  \"sales_rep_ssn\": (136, 15),\n",
    "  \"sales_rep_first_name\": (151, 15),\n",
    "  \"sales_rep_last_name\": (166, 15),\n",
    "  \"sales_rep_address\": (181, 40),\n",
    "  \"sales_rep_city\": (221, 20),\n",
    "  \"sales_rep_state\": (241, 2),\n",
    "  \"sales_rep_zip\": (243, 5),\n",
    "  \"shipping_address_attention\": (248, 30),\n",
    "  \"shipping_address_address\": (278, 40),\n",
    "  \"shipping_address_city\": (318, 20),\n",
    "  \"shipping_address_state\": (338, 2),\n",
    "  \"shipping_address_zip\": (340, 5),\n",
    "  \"product_id\": (345, 40),\n",
    "  \"product_quantity\": (385, 5),\n",
    "  \"product_sold_price\": (390, 20)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Exercise #1.A\n",
    "\n",
    "Implement your solution in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use this cell to complete your solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #1.B - Ingest Tab-Separted File</h2>\n",
    "\n",
    "**In this step you will need to:**\n",
    "1. Use the variable **`batch_2018_path`**, and investigate the 2018 batch file, if needed.\n",
    "2. Configure a **`DataFrameReader`** to ingest the tab-separated file identified by **`batch_2018_path`**\n",
    "3. Add a new column, **`ingest_file_name`**, which is the name of the file from which the data was read from - note this should not be hard coded.\n",
    "4. Add a new column, **`ingested_at`**, which is a timestamp of when the data was ingested as a DataFrame - note this should not be hard coded.\n",
    "5. **Append** the corresponding **`DataFrame`** to the previously created datasets specified by **`batch_target_path`**\n",
    "\n",
    "**Additional Requirements**\n",
    "* Any **\"null\"** strings in the CSV file should be replaced with the SQL value **null**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Exercise #1.b\n",
    "\n",
    "Implement your solution in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use this cell to complete your solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #1.C - Ingest Comma-Separted File</h2>\n",
    "\n",
    "**In this step you will need to:**\n",
    "1. Use the variable **`batch_2019_path`**, and investigate the 2019 batch file, if needed.\n",
    "2. Configure a **`DataFrameReader`** to ingest the comma-separated file identified by **`batch_2019_path`**\n",
    "3. Add a new column, **`ingest_file_name`**, which is the name of the file from which the data was read from - note this should not be hard coded.\n",
    "4. Add a new column, **`ingested_at`**, which is a timestamp of when the data was ingested as a DataFrame - note this should not be hard coded.\n",
    "5. **Append** the corresponding **`DataFrame`** to the previously created dataset specified by **`batch_target_path`**<br/>\n",
    "   Note: The column names in this dataset must be updated to conform to the schema defined for Exercise #2.A - there are several strategies for this:\n",
    "   * Provide a schema that alters the names upon ingestion\n",
    "   * Manually rename one column at a time\n",
    "   * Use **`fixed_width_column_defs`** programatically rename one column at a time\n",
    "   * Use transformations found in the **`DataFrame`** class to rename all columns in one operation\n",
    "\n",
    "**Additional Requirements**\n",
    "* Any **\"null\"** strings in the CSV file should be replaced with the SQL value **null**<br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Exercise #1.C\n",
    "\n",
    "Implement your solution in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use this cell to complete your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ead50db5f464cadce5ba018366328249412102842550c0c50bef2a2fd10d90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
