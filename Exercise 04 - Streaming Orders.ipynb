{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4 - Streaming Orders\n",
    "\n",
    "With our four historical datasets properly loaded, we can now begin to process the \"current\" orders.\n",
    "\n",
    "In this case, the new \"system\" is landing one JSON file per order into cloud storage.\n",
    "\n",
    "We can process these JSON files as a stream of orders under the assumption that new orders are continually added to this dataset.\n",
    "\n",
    "In order to keep this project simple, we have reduced the \"stream\" of orders to just the first few hours of 2020 and will be throttling that stream to only one file per iteration.\n",
    "\n",
    "This exercise is broken up into 3 steps:\n",
    "* Exercise 4.A - Use Database\n",
    "* Exercise 4.B - Stream-Append Orders\n",
    "* Exercise 4.C - Stream-Append Line Items\n",
    "\n",
    "## Some Friendly Advice...\n",
    "\n",
    "Each record is a JSON object with roughly the following structure:\n",
    "\n",
    "* **`customerID`**\n",
    "* **`orderId`**\n",
    "* **`products`**\n",
    "  * array\n",
    "    * **`productId`**\n",
    "    * **`quantity`**\n",
    "    * **`soldPrice`**\n",
    "* **`salesRepId`**\n",
    "* **`shippingAddress`**\n",
    "  * **`address`**\n",
    "  * **`attention`**\n",
    "  * **`city`**\n",
    "  * **`state`**\n",
    "  * **`zip`**\n",
    "* **`submittedAt`**\n",
    "\n",
    "As you ingest this data, it will need to be transformed to match the existing **`orders`** table's schema and the **`line_items`** table's schema.\n",
    "\n",
    "Before attempting to ingest the data as a stream, we highly recomend that you start with a static **`DataFrame`** so that you can iron out the various kinks:\n",
    "* Renaming and flattening columns\n",
    "* Exploding the products array\n",
    "* Parsing the **`submittedAt`** column into a **`timestamp`**\n",
    "* Conforming to the **`orders`** and **`line_items`** schemas - because these are Delta tables, appending to them will fail if the schemas are not correct\n",
    "\n",
    "Furthermore, creating a stream from JSON files will first require you to specify the schema - you can \"cheat\" and infer that schema from some of the JSON files before starting the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_db = \"learning_db\"\n",
    "orders_table = \"orders\"\n",
    "products_table = \"products\"\n",
    "line_items_table = \"line_items\"\n",
    "stream_path = \"data/orders/stream\"\n",
    "orders_checkpoint_path = \"checkpoint/orders\"\n",
    "line_items_checkpoint_path = \"checkpoint/line_items\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(\"checkpoint\"):\n",
    "    os.mkdir(\"checkpoint\")\n",
    "\n",
    "if os.path.exists(orders_checkpoint_path):\n",
    "    shutil.rmtree(orders_checkpoint_path, True)\n",
    "\n",
    "if os.path.exists(line_items_checkpoint_path):\n",
    "    shutil.rmtree(line_items_checkpoint_path, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #4.A - Use Database</h2>\n",
    "\n",
    "Each notebook uses a different Spark session and will initially use the **`default`** database.\n",
    "\n",
    "As in the previous exercise, we can avoid contention to commonly named tables by using our user-specific database.\n",
    "\n",
    "**In this step you will need to:**\n",
    "* Use the database identified by the variable **`user_db`** so that any tables created in this notebook are **NOT** added to the **`default`** database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use this cell to complete your solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #4.B - Stream-Append Orders</h2>\n",
    "\n",
    "Every JSON file ingested by our stream representes one order and the enumerated list of products purchased in that order.\n",
    "\n",
    "Our goal is simple, ingest the data, transform it as required by the **`orders`** table's schema, and append these new records to our existing table.\n",
    "\n",
    "**In this step you will need to:**\n",
    "\n",
    "* Ingest the stream of JSON files:\n",
    "  * Start a stream from the path identified by **`stream_path`**.\n",
    "  * Using the **`maxFilesPerTrigger`** option, throttle the stream to process only one file per iteration.\n",
    "  * Add the ingest meta data (same as with our other datasets):\n",
    "    * **`ingested_at`**:**`timestamp`**\n",
    "    * **`ingest_file_name`**:**`string`**\n",
    "  * Properly parse the **`submitted_at`**  as a valid **`timestamp`**\n",
    "  * Add the column **`submitted_yyyy_mm`** usinge the format \"**yyyy-MM**\"\n",
    "  * Make any other changes required to the column names and data types so that they conform to the **`orders`** table's schema\n",
    "\n",
    "* Write the stream to a Delta **table**.:\n",
    "  * The table's format should be \"**delta**\"\n",
    "  * Partition the data by the column **`submitted_yyyy_mm`**\n",
    "  * Records must be appended to the table identified by the variable **`orders_table`**\n",
    "  * The query must be named the same as the table, identified by the variable **`orders_table`**\n",
    "  * The query must use the checkpoint location identified by the variable **`orders_checkpoint_path`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Exercise #4.B\n",
    "\n",
    "Implement your solution in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use this cell to complete your solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #4.C - Stream-Append Line Items</h2>\n",
    "\n",
    "The same JSON file we processed in the previous stream also contains the line items which we now need to extract and append to the existing **`line_items`** table.\n",
    "\n",
    "Just like before, our goal is simple, ingest the data, transform it as required by the **`line_items`** table's schema, and append these new records to our existing table.\n",
    "\n",
    "Note: we are processing the same stream twice - there are other patterns to do this more efficiently, but for this exercise, we want to keep the design simple.<br/>\n",
    "The good news here is that you can copy most of the code from the previous step to get you started here.\n",
    "\n",
    "**In this step you will need to:**\n",
    "\n",
    "* Ingest the stream of JSON files:\n",
    "  * Start a stream from the path identified by **`stream_path`**.\n",
    "  * Using the **`maxFilesPerTrigger`** option, throttle the stream to process only one file per iteration.\n",
    "  * Add the ingest meta data (same as with our other datasets):\n",
    "    * **`ingested_at`**:**`timestamp`**\n",
    "    * **`ingest_file_name`**:**`string`**\n",
    "  * Make any other changes required to the column names and data types so that they conform to the **`line_items`** table's schema\n",
    "    * The most significant transformation will be to the **`products`** column.\n",
    "    * The **`products`** column is an array of elements and needs to be exploded (see **`pyspark.sql.functions`**)\n",
    "    * One solution would include:\n",
    "      1. Select **`order_id`** and explode **`products`** while renaming it to **`product`**.\n",
    "      2. Flatten the **`product`** column's nested values.\n",
    "      3. Add the ingest meta data (**`ingest_file_name`** and **`ingested_at`**).\n",
    "      4. Convert data types as required by the **`line_items`** table's schema.\n",
    "\n",
    "* Write the stream to a Delta sink:\n",
    "  * The sink's format should be \"**delta**\"\n",
    "  * Records must be appended to the table identified by the variable **`line_items_table`**\n",
    "  * The query must be named the same as the table, identified by the variable **`line_items_table`**\n",
    "  * The query must use the checkpoint location identified by the variable **`line_items_checkpoint_path`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Exercise #4.C\n",
    "\n",
    "Implement your solution in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use this cell to complete your solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ead50db5f464cadce5ba018366328249412102842550c0c50bef2a2fd10d90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
